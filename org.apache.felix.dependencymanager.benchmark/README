Installation:
============

- see toplevel README on how to import dependencymanager into bndtools

How to launch the stress test under bndtools:
============================================

The stress test performs two kind of tests on DM and parallel DM.

1) first kind of tests: starts/stops several times each tested bundle (SCR/DM/Parallel DM). When
started, the test bundle is expected to register/unregister several services. And no processing is
done at all in each component activation/deactivation methods (start/stop methods).

2) second kind of tests: same as before, but some processing is done in each component start/stop methods.

To launch the stress test under BndTools, click on the bnd.bnd file of the
"org.apache.felix.dm.benchmark" project, then click on "Run", then in "Run OSGi". 

You should see something like that in the eclipse console:

-----------------------------------------------------------------------------------------------------------------
g! Starting benchmarks (each tested bundle will create 210 components).

	[Starting benchmarks with no processing done in components start/stop methods]

Benchmarking bundle: org.apache.felix.dependencymanager.benchmark.dependencymanager ...............
-> results in nanos: [85,865,048 | 98,607,629 | 127,745,901 | 148,501,547 | 358,559,677]

Benchmarking bundle: org.apache.felix.dependencymanager.benchmark.dependencymanager.parallel ...............
-> results in nanos: [62,455,799 | 65,343,664 | 69,237,440 | 76,452,266 | 82,839,040]

	[Starting benchmarks with processing done in components start/stop methods]

Benchmarking bundle: org.apache.felix.dependencymanager.benchmark.dependencymanager .....
-> results in nanos: [1,950,203,709 | 1,969,656,854 | 2,004,276,779 | 2,034,080,987 | 2,104,094,347]

Benchmarking bundle: org.apache.felix.dependencymanager.benchmark.dependencymanager.parallel .....
-> results in nanos: [498,377,362 | 515,902,895 | 524,460,514 | 542,052,780 | 606,619,382]
-----------------------------------------------------------------------------------------------------------------

You can also possibly run the same test using optimized DM filter indices.
To do so, add the following jvm system parameters in the bnd.bnd "-runproperties" option:

-runproperties:  \
	ds.loglevel=warn,\
	org.osgi.framework.bootdelegation='sun.*,com.sun.*,org.netbeans.*',\
   -Dorg.apache.felix.dependencymanager.filterindex=objectClass,id

You should then observe better performance results

How to interpret results:
========================

for each tested bundle, the time spent is displayed in nanos.
for example: 

  results=165645599,240534693,250335703,276343059,458505648

Here is how to interpret the results: when testing a bundle, the benchmark controller starts/stops
it many times, then the elapsed time used to start the bundle, activate/deactivate all services, and
stop the bundle is recorded in a list. Then this list is sorted: the first entry is the fastest
execution time, the last entry is the slowest. the middle one is the average. We display the first
entry (fastest), the entry at 1/4 of the list, the middle of the list, the entry at 3/4 of the list,
and the last entry (slowest time).

We don't do an average, because usually, when running benchmark, some measurements don't reflect
reality, especially, when there is a full GC or when the JVM is warming up. (we actually do the same
as in Java Chronicle: https://github.com/peter-lawrey/Java-Chronicle). 

Stress test scenario description
--------------------------------

For sake of simplicity, a simple scenario domain is used (actually, this example domain has been
inspired from the "Java8 Lambdas" book, O'reilly): We have the following services: 

"Artist" service: An Artist is an individual or group of musicians, who creates some "Albums". One
Artist service depends on several Album services. 

"Album" service: is a single release of musics, comprising several music Tracks. One Album depends
on several Track services. 

"Track" service: A piece of music.

By default, a scenario consists in registering 5 artists, each artist having 10 albums, and each
album having 5 music tracks (~ 300 services).

The scenario is implemented in the following bundles

- org.apache.felix.dm.benchmark.scenario: defines the interfaces.
- org.apache.felix.dm.benchmark.scenario.impl: defines the basic implementations for the services.

The Scenario Controller (see
org.apache.felix.dm.benchmark.scenario/org.apache.felix.dm.benchmark.scenario.impl) is in charge of
starting/stopping many times some DI specific bundles (DM4, SCR), which are expected to create all
expected Artist/Album/Track services. 

By default, when a tested bundle is started, it will create several Artists (see Artists.ARTISTS
constant). each Artist depends on several Albums (see Artists.ALBUMS constant), and each Album
depends on several music Tracks (see Artists.TRACKS constants).

Test bundles (DM, parallel DM)
==============================

- org.apache.felix.dm.benchmark.dependencymanager: 
It contains a simple activator, which creates the various services using dependency manager API.

- org.apache.felix.dm.benchmark.dependencymanager.parallel: 
same as above, but using parallel dependency manager where components dependency management and
components activation/deactivation processing is performed concurrently, using a fixed thread pool.

The org.apache.felix.dm.benchmark.controller bundle, when starting, first stops all tested bundles.
Then for each one, it performs the following test (multiple times):

- register the ScenarioController service (using the OSGi BundleContext API)
- start the tested bundle
- wait for all expected services to be registered
- unregister the ScenarioController service (this will trigger a full deactivation of services,
because all services are expected to depend on it). 
- wait for all expected services to be unregistered
- stop the tested bundle.

All the elapsed time (nanoseconds) used to execute each iteration is then recorded in a list.
When enough iterations are done, the list is sorted (that is : the first entry in the list
corresponds to the fastest execution time, and the last entry corresponds to the slowest execution
time). Then, we display some meaningful entries in the list (like the first entry, the entry in the
middle of the list (average), and the last entry (slowest).
